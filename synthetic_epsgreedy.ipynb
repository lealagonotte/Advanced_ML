{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1de8525",
   "metadata": {},
   "source": [
    "# We will study the algorithm on the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5e9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa392daa",
   "metadata": {},
   "source": [
    "# Load the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76549381",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.read_csv('synthetic_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b4e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  UserID  MovieID  rating  binary_rating\n",
      "0           0       0        0       5              1\n",
      "1           1       0        1       1              0\n",
      "2           2       0        2       4              1\n",
      "3           3       0        3       5              1\n",
      "4           4       0        4       1              0\n",
      "5000000\n"
     ]
    }
   ],
   "source": [
    "print(synthetic_data.head())\n",
    "print(len(synthetic_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb93027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpsilonGreedyRecommender:\n",
    "    def __init__(self, df, epsilon=1.0, decay=True):\n",
    "        \"\"\"\n",
    "        df: DataFrame with columns ['user_id','movie_id','rating']\n",
    "        epsilon: initial exploration rate\n",
    "        decay: if True, apply decay schedule epsilon ~ t^(-1/3)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.movie_ids = df['movie_id'].unique().tolist()\n",
    "        self.K = len(self.movie_ids)\n",
    "\n",
    "        self.epsilon_0 = epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.t = 0\n",
    "\n",
    "        # Estimated mean rewards (ratings)\n",
    "        self.q_values = {m_id: 0.0 for m_id in self.movie_ids}\n",
    "        self.attempts = {m_id: 0 for m_id in self.movie_ids}\n",
    "\n",
    "        # Prepare a dictionary of ratings per movie\n",
    "        self.ratings_dict = df.groupby('movie_id')['rating'].apply(list).to_dict()\n",
    "\n",
    "        # For regret tracking\n",
    "        self.optimal_mean = df.groupby('movie_id')['rating'].mean().max()\n",
    "        self.regret_list = []\n",
    "\n",
    "    def select_movie(self):\n",
    "        \"\"\"Epsilon-greedy selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.movie_ids)\n",
    "        else:\n",
    "            return max(self.q_values, key=self.q_values.get)\n",
    "\n",
    "    def get_reward(self, movie_id):\n",
    "        \"\"\"Sample a rating from the movie's list\"\"\"\n",
    "        return random.choice(self.ratings_dict[movie_id])\n",
    "\n",
    "    def update_scores(self, movie_id, reward):\n",
    "        \"\"\"Incremental update of estimated mean rating\"\"\"\n",
    "        self.attempts[movie_id] += 1\n",
    "        n = self.attempts[movie_id]\n",
    "        old_q = self.q_values[movie_id]\n",
    "        self.q_values[movie_id] = old_q + (reward - old_q) / n\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Perform one iteration of epsilon-greedy\"\"\"\n",
    "        self.t += 1\n",
    "        if self.decay:\n",
    "            self.epsilon = self.epsilon_0 * (self.t ** (-1/3))\n",
    "\n",
    "        movie = self.select_movie()\n",
    "        reward = self.get_reward(movie)\n",
    "        self.update_scores(movie, reward)\n",
    "\n",
    "        # Update regret\n",
    "        instant_regret = self.optimal_mean - reward\n",
    "        if self.regret_list:\n",
    "            self.regret_list.append(self.regret_list[-1] + instant_regret)\n",
    "        else:\n",
    "            self.regret_list.append(instant_regret)\n",
    "\n",
    "        return movie, reward\n",
    "\n",
    "# --- Example usage ---\n",
    "\n",
    "# Suppose you have a smaller subset for testing\n",
    "df = interactions_df.sample(n=500_000, random_state=42)  # sample for speed\n",
    "\n",
    "recommender = EpsilonGreedyRecommender(df, epsilon=1.0, decay=True)\n",
    "\n",
    "T = 50_000  # number of iterations\n",
    "for _ in range(T):\n",
    "    recommender.step()\n",
    "\n",
    "# Plot cumulative regret\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(recommender.regret_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.title(\"Epsilon-Greedy: Cumulative Regret over Iterations\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
